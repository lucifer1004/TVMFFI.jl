<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · TVMFFI.jl</title><meta name="title" content="Home · TVMFFI.jl"/><meta property="og:title" content="Home · TVMFFI.jl"/><meta property="twitter:title" content="Home · TVMFFI.jl"/><meta name="description" content="Documentation for TVMFFI.jl."/><meta property="og:description" content="Documentation for TVMFFI.jl."/><meta property="twitter:description" content="Documentation for TVMFFI.jl."/><meta property="og:url" content="https://lucifer1004.github.io/TVMFFI.jl/"/><meta property="twitter:url" content="https://lucifer1004.github.io/TVMFFI.jl/"/><link rel="canonical" href="https://lucifer1004.github.io/TVMFFI.jl/"/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>TVMFFI.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#Features"><span>Features</span></a></li><li><a class="tocitem" href="#Installation"><span>Installation</span></a></li><li><a class="tocitem" href="#Quick-Start"><span>Quick Start</span></a></li><li><a class="tocitem" href="#Documentation"><span>Documentation</span></a></li><li><a class="tocitem" href="#Performance"><span>Performance</span></a></li><li><a class="tocitem" href="#Architecture"><span>Architecture</span></a></li><li><a class="tocitem" href="#Design-Philosophy"><span>Design Philosophy</span></a></li><li><a class="tocitem" href="#License"><span>License</span></a></li></ul></li><li><a class="tocitem" href="api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/lucifer1004/TVMFFI.jl/blob/main/README.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="TVMFFI.jl"><a class="docs-heading-anchor" href="#TVMFFI.jl">TVMFFI.jl</a><a id="TVMFFI.jl-1"></a><a class="docs-heading-anchor-permalink" href="#TVMFFI.jl" title="Permalink"></a></h1><p><a href="https://lucifer1004.github.io/TVMFFI.jl/dev/"><img src="https://img.shields.io/badge/docs-dev-blue.svg" alt="Dev"/></a> <a href="https://github.com/lucifer1004/TVMFFI.jl/actions/workflows/CI.yml?query=branch%3Amain"><img src="https://github.com/lucifer1004/TVMFFI.jl/actions/workflows/CI.yml/badge.svg?branch=main" alt="Build Status"/></a> <a href="https://codecov.io/gh/lucifer1004/TVMFFI.jl"><img src="https://codecov.io/gh/lucifer1004/TVMFFI.jl/branch/main/graph/badge.svg" alt="Coverage"/></a></p><p>Julia bindings for the TVM FFI (Foreign Function Interface).</p><h2 id="Features"><a class="docs-heading-anchor" href="#Features">Features</a><a id="Features-1"></a><a class="docs-heading-anchor-permalink" href="#Features" title="Permalink"></a></h2><p>TVMFFI.jl provides a complete, idiomatic Julia interface to TVM&#39;s C API:</p><h3 id="Core-Functionality"><a class="docs-heading-anchor" href="#Core-Functionality">✅ Core Functionality</a><a id="Core-Functionality-1"></a><a class="docs-heading-anchor-permalink" href="#Core-Functionality" title="Permalink"></a></h3><ul><li><strong>Device Management</strong>: CPU, CUDA, OpenCL, Vulkan, Metal, ROCm support</li><li><strong>Data Types</strong>: Full DLPack integration with automatic type conversion</li><li><strong>Zero-Copy TensorViews</strong>: Efficient data exchange via <code>TensorView</code></li><li><strong>Error Handling</strong>: TVM errors automatically mapped to Julia exceptions</li><li><strong>Strings &amp; Bytes</strong>: Small string optimization, heap allocation for large strings</li></ul><h3 id="Function-System"><a class="docs-heading-anchor" href="#Function-System">✅ Function System</a><a id="Function-System-1"></a><a class="docs-heading-anchor-permalink" href="#Function-System" title="Permalink"></a></h3><ul><li><strong>Call TVM Functions</strong>: <code>get_global_func()</code> to retrieve and call TVM functions</li><li><strong>Register Julia Functions</strong>: <code>register_global_func()</code> exposes Julia functions to TVM</li><li><strong>Automatic Conversion</strong>: Seamless conversion between Julia and TVM types</li><li><strong>Exception Safety</strong>: Julia errors are properly translated to TVM errors</li></ul><h3 id="Object-System"><a class="docs-heading-anchor" href="#Object-System">✅ Object System</a><a id="Object-System-1"></a><a class="docs-heading-anchor-permalink" href="#Object-System" title="Permalink"></a></h3><ul><li><strong>Type Registration</strong>: <code>@register_object</code> macro for wrapping TVM types in Julia</li><li><strong>Reflection API</strong>: <code>get_type_info()</code>, <code>get_fields()</code>, <code>get_methods()</code> for introspection</li><li><strong>Property Access</strong>: Automatic field/method access via <code>obj.field</code> and <code>obj.method()</code></li><li><strong>Constructors</strong>: Direct <code>TypeName(args...)</code> syntax for types with <code>__ffi_init__</code></li><li><strong>Reference Counting</strong>: Automatic memory management via finalizers</li></ul><h3 id="Module-System"><a class="docs-heading-anchor" href="#Module-System">✅ Module System</a><a id="Module-System-1"></a><a class="docs-heading-anchor-permalink" href="#Module-System" title="Permalink"></a></h3><ul><li><strong>Load Modules</strong>: <code>load_module()</code> to load compiled TVM modules</li><li><strong>Query Functions</strong>: <code>mod[&quot;function_name&quot;]</code> or <code>get_function(mod, name)</code></li><li><strong>System Library</strong>: <code>system_lib()</code> for statically linked modules</li><li><strong>Module Introspection</strong>: <code>inspect_source()</code>, <code>get_module_kind()</code>, <code>implements_function()</code></li><li><strong>Module Export</strong>: <code>write_to_file()</code> to save compiled modules</li><li><strong>Module Caching</strong>: Efficient global function caching</li></ul><h2 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h2><pre><code class="language-julia hljs">using Pkg
Pkg.add(&quot;TVMFFI&quot;)</code></pre><p>Or for the latest development version:</p><pre><code class="language-julia hljs">Pkg.add(url=&quot;https://github.com/lucifer1004/TVMFFI.jl&quot;)</code></pre><h2 id="Quick-Start"><a class="docs-heading-anchor" href="#Quick-Start">Quick Start</a><a id="Quick-Start-1"></a><a class="docs-heading-anchor-permalink" href="#Quick-Start" title="Permalink"></a></h2><h3 id="Basic-Usage"><a class="docs-heading-anchor" href="#Basic-Usage">Basic Usage</a><a id="Basic-Usage-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-Usage" title="Permalink"></a></h3><pre><code class="language-julia hljs">using TVMFFI

# Check TVM FFI version
v = tvm_ffi_version()
println(&quot;TVM FFI Version: $v&quot;)  # e.g., &quot;0.1.2&quot;

# Create devices
cpu_dev = cpu(0)
cuda_dev = cuda(0)

# Data types
dt = DLDataType(Float32)
println(string(dt))  # &quot;float32&quot;</code></pre><h3 id="Working-with-Arrays-(Zero-Copy)"><a class="docs-heading-anchor" href="#Working-with-Arrays-(Zero-Copy)">Working with Arrays (Zero-Copy)</a><a id="Working-with-Arrays-(Zero-Copy)-1"></a><a class="docs-heading-anchor-permalink" href="#Working-with-Arrays-(Zero-Copy)" title="Permalink"></a></h3><pre><code class="language-julia hljs"># 1. CPU Arrays
x = Float32[1, 2, 3, 4, 5]
holder = TensorView(x)

# 2. GPU Arrays (requires CUDA.jl, Metal.jl, etc.)
# Automatically detects device type and handles pointers correctly
using CUDA
x_gpu = CuArray(Float32[1, 2, 3])
holder_gpu = TensorView(x_gpu)

# 3. Call TVM functions
# Arrays are automatically converted to DLTensor
result = some_tvm_func(x_gpu)</code></pre><h3 id="Loading-Modules"><a class="docs-heading-anchor" href="#Loading-Modules">Loading Modules</a><a id="Loading-Modules-1"></a><a class="docs-heading-anchor-permalink" href="#Loading-Modules" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Load a compiled module
mod = load_module(&quot;path/to/module.so&quot;)

# Get and call functions
my_func = mod[&quot;function_name&quot;]
output = my_func(input1, input2)</code></pre><h3 id="Registering-Julia-Functions"><a class="docs-heading-anchor" href="#Registering-Julia-Functions">Registering Julia Functions</a><a id="Registering-Julia-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Registering-Julia-Functions" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Define a Julia function
function my_add(x::Int64, y::Int64)
    return x + y
end

# Register it to TVM
register_global_func(&quot;julia.my_add&quot;, my_add)

# Call it from TVM
func = get_global_func(&quot;julia.my_add&quot;)
result = func(Int64(10), Int64(20))  # Returns 30</code></pre><h3 id="Working-with-TVM-Objects"><a class="docs-heading-anchor" href="#Working-with-TVM-Objects">Working with TVM Objects</a><a id="Working-with-TVM-Objects-1"></a><a class="docs-heading-anchor-permalink" href="#Working-with-TVM-Objects" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Register a TVM type for use in Julia
@register_object &quot;testing.TestCxxClassBase&quot; TestCxxClassBase

# Create instances (if the type has __ffi_init__)
obj = TestCxxClassBase(Int64(42), Int32(10))

# Access fields via reflection
println(obj.v_i64)  # 42
println(obj.v_i32)  # 10

# Modify fields
obj.v_i64 = Int64(100)
obj.v_i32 = Int32(20)

# Type introspection
println(type_key(TestCxxClassBase))   # &quot;testing.TestCxxClassBase&quot;
println(type_index(TestCxxClassBase)) # Runtime type index</code></pre><h2 id="Documentation"><a class="docs-heading-anchor" href="#Documentation">Documentation</a><a id="Documentation-1"></a><a class="docs-heading-anchor-permalink" href="#Documentation" title="Permalink"></a></h2><p>For full API documentation, see <a href="https://lucifer1004.github.io/TVMFFI.jl/dev/">Documentation</a>.</p><ul><li><strong><a href="https://lucifer1004.github.io/TVMFFI.jl/dev/api/">API Reference</a></strong>: Complete list of exported functions and types.</li></ul><h2 id="Performance"><a class="docs-heading-anchor" href="#Performance">Performance</a><a id="Performance-1"></a><a class="docs-heading-anchor-permalink" href="#Performance" title="Permalink"></a></h2><p>FFI overhead has been carefully measured. Julia TVMFFI is <strong>significantly faster than Python</strong> in most scenarios.</p><h3 id="Julia-vs-Python-Comparison"><a class="docs-heading-anchor" href="#Julia-vs-Python-Comparison">Julia vs Python Comparison</a><a id="Julia-vs-Python-Comparison-1"></a><a class="docs-heading-anchor-permalink" href="#Julia-vs-Python-Comparison" title="Permalink"></a></h3><table><tr><th style="text-align: right">Operation</th><th style="text-align: right">Julia</th><th style="text-align: right">Python</th><th style="text-align: right">Speedup</th></tr><tr><td style="text-align: right">CPU broadcast add</td><td style="text-align: right">3 ns</td><td style="text-align: right">207 ns</td><td style="text-align: right"><strong>67x faster</strong></td></tr><tr><td style="text-align: right">TVM NOP (no args)</td><td style="text-align: right">11 ns</td><td style="text-align: right">72 ns</td><td style="text-align: right"><strong>6.5x faster</strong></td></tr><tr><td style="text-align: right">TVM NOP (pre-converted)</td><td style="text-align: right">47 ns</td><td style="text-align: right">72 ns</td><td style="text-align: right"><strong>1.5x faster</strong></td></tr><tr><td style="text-align: right">TVM autodlpack (CPU)</td><td style="text-align: right">30 ns</td><td style="text-align: right">298 ns</td><td style="text-align: right"><strong>10x faster</strong></td></tr><tr><td style="text-align: right"><strong>TVM autodlpack (GPU)</strong></td><td style="text-align: right"><strong>580 ns</strong></td><td style="text-align: right"><strong>902 ns</strong></td><td style="text-align: right"><strong>1.6x faster</strong></td></tr><tr><td style="text-align: right">CUDA stream query</td><td style="text-align: right">19 ns</td><td style="text-align: right">85 ns</td><td style="text-align: right"><strong>4.5x faster</strong></td></tr></table><h3 id="FFI-Overhead-Summary"><a class="docs-heading-anchor" href="#FFI-Overhead-Summary">FFI Overhead Summary</a><a id="FFI-Overhead-Summary-1"></a><a class="docs-heading-anchor-permalink" href="#FFI-Overhead-Summary" title="Permalink"></a></h3><table><tr><th style="text-align: right">Operation</th><th style="text-align: right">Time</th><th style="text-align: right">Allocations</th></tr><tr><td style="text-align: right">TVM func() empty</td><td style="text-align: right">11 ns</td><td style="text-align: right">2 (64 B)</td></tr><tr><td style="text-align: right">TVM func(Int64)</td><td style="text-align: right">20 ns</td><td style="text-align: right">2 (64 B)</td></tr><tr><td style="text-align: right">TVM func(Array) autodlpack</td><td style="text-align: right">30 ns</td><td style="text-align: right">3 (144 B)</td></tr><tr><td style="text-align: right">TVM func(CuArray) autodlpack</td><td style="text-align: right">200 ns</td><td style="text-align: right">3 (144 B)</td></tr></table><p><strong>Note</strong>: Array calls use zero-copy <code>TensorView</code> - overhead is <strong>O(1)</strong> regardless of array size.</p><h3 id="Overhead-Breakdown"><a class="docs-heading-anchor" href="#Overhead-Breakdown">Overhead Breakdown</a><a id="Overhead-Breakdown-1"></a><a class="docs-heading-anchor-permalink" href="#Overhead-Breakdown" title="Permalink"></a></h3><p>For a typical scalar function call (~260 ns):</p><pre><code class="nohighlight hljs">ccall + callback dispatch:  232 ns  (89.5%)  ← C-side overhead
Args Vector allocation:      12 ns  ( 4.7%)
TVMAny conversion:            7 ns  ( 2.6%)
Result Ref allocation:        6 ns  ( 2.5%)
raw_data extraction:          2 ns  ( 0.7%)</code></pre><p><strong>Key insight</strong>: 89.5% of overhead is in C-side callback dispatch, not Julia.</p><h3 id="Practical-Impact"><a class="docs-heading-anchor" href="#Practical-Impact">Practical Impact</a><a id="Practical-Impact-1"></a><a class="docs-heading-anchor-permalink" href="#Practical-Impact" title="Permalink"></a></h3><table><tr><th style="text-align: right">Workload</th><th style="text-align: right">FFI Overhead</th><th style="text-align: right">Recommendation</th></tr><tr><td style="text-align: right">Inference (1ms+)</td><td style="text-align: right">&lt; 0.03%</td><td style="text-align: right">✅ Negligible</td></tr><tr><td style="text-align: right">Micro-ops (1μs)</td><td style="text-align: right">~20-30%</td><td style="text-align: right">⚠️ Consider batching</td></tr><tr><td style="text-align: right">Hot loops (100ns)</td><td style="text-align: right">Dominates</td><td style="text-align: right">❌ Use raw ccall</td></tr></table><h3 id="Running-Benchmarks"><a class="docs-heading-anchor" href="#Running-Benchmarks">Running Benchmarks</a><a id="Running-Benchmarks-1"></a><a class="docs-heading-anchor-permalink" href="#Running-Benchmarks" title="Permalink"></a></h3><pre><code class="language-bash hljs">cd benchmarks
julia --project=. -e &#39;using Pkg; Pkg.develop(path=&quot;..&quot;); Pkg.instantiate()&#39;
julia --project=. ffi_overhead.jl
julia --project=. microbenchmarks.jl</code></pre><h3 id="Known-Limitation:-GPU-Benchmarking-with-BenchmarkTools"><a class="docs-heading-anchor" href="#Known-Limitation:-GPU-Benchmarking-with-BenchmarkTools">Known Limitation: GPU Benchmarking with BenchmarkTools</a><a id="Known-Limitation:-GPU-Benchmarking-with-BenchmarkTools-1"></a><a class="docs-heading-anchor-permalink" href="#Known-Limitation:-GPU-Benchmarking-with-BenchmarkTools" title="Permalink"></a></h3><p><strong><code>@benchmark</code> cannot be used with functions returning new GPU arrays</strong> - it will segfault.</p><pre><code class="language-julia hljs"># ❌ CRASHES: BenchmarkTools&#39; internal loop discards return values
@benchmark my_gpu_func($arr)

# ❌ STILL CRASHES: @benchmark only saves first iteration&#39;s result
local result
@benchmark result = my_gpu_func($arr)

# ✅ WORKS: Manual timing with explicit result retention
local result
for _ in 1:N
    result = my_gpu_func(arr)  # Each result kept alive until next iteration
    CUDA.synchronize()
end</code></pre><p><strong>Root cause</strong>: BenchmarkTools&#39; generated code only saves the first iteration&#39;s return value. Subsequent iterations in the internal loop discard results, then <code>gcscrub()</code> triggers GC which crashes in CUDA.jl finalizers.</p><p>This is a known Julia ecosystem issue. For GPU benchmarks, use manual timing or <code>CUDA.@time</code>. See <a href="https://cuda.juliagpu.org/stable/development/profiling/">CUDA.jl profiling docs</a>.</p><h2 id="Architecture"><a class="docs-heading-anchor" href="#Architecture">Architecture</a><a id="Architecture-1"></a><a class="docs-heading-anchor-permalink" href="#Architecture" title="Permalink"></a></h2><pre><code class="nohighlight hljs">TVMFFI/
├── src/
│   ├── LibTVMFFI.jl          # Low-level C bindings
│   ├── TVMFFI.jl             # Main module
│   ├── any.jl                # TVMAny/TVMAnyView ownership containers
│   ├── conversion.jl         # ABI boundary layer (to_tvm_any, take_value, copy_value)
│   ├── device.jl             # Device abstractions
│   ├── dtype.jl              # Data type handling
│   ├── dlpack.jl             # DLPack zero-copy tensor exchange
│   ├── error.jl              # Error handling
│   ├── function.jl           # Function calls &amp; registration
│   ├── gpuarrays_support.jl  # GPU array integration (via DLPack.jl)
│   ├── module.jl             # Module loading
│   ├── object.jl             # Object system
│   ├── string.jl             # String/Bytes types
│   ├── tensor.jl             # DLTensor support
│   └── utils.jl              # Utility functions
├── ext/                      # Package extensions
│   ├── CUDAExt.jl            # NVIDIA CUDA support
│   ├── AMDGPUExt.jl          # AMD ROCm support
│   └── MetalExt.jl           # Apple Metal support
├── test/                     # Comprehensive test suite
├── docs/                     # Documentation
├── examples/                 # Usage examples
├── benchmarks/               # Performance benchmarks
├── fixtures/                 # Test fixtures and build files
├── build/                    # Build artifacts
├── Project.toml              # Julia package configuration
├── AGENTS.md                 # Agent guide (technical documentation)
├── LICENSE                   # Apache 2.0 license
└── README.md                 # This file</code></pre><h3 id="GPU-Support"><a class="docs-heading-anchor" href="#GPU-Support">GPU Support</a><a id="GPU-Support-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-Support" title="Permalink"></a></h3><p>GPU arrays use lightweight <code>TensorView</code> (same as CPU), achieving <strong>4.5x faster</strong> FFI calls than Python:</p><ul><li><strong>CUDA</strong>: TVMFFI&#39;s CUDAExt (device detection, sync callbacks, tensor views)</li><li><strong>AMD ROCm</strong>: TVMFFI&#39;s AMDGPUExt</li><li><strong>Apple Metal</strong>: TVMFFI&#39;s MetalExt</li></ul><p>All GPU extensions automatically register cleanup hooks to prevent finalizer order issues.</p><h2 id="Design-Philosophy"><a class="docs-heading-anchor" href="#Design-Philosophy">Design Philosophy</a><a id="Design-Philosophy-1"></a><a class="docs-heading-anchor-permalink" href="#Design-Philosophy" title="Permalink"></a></h2><p>Following Linus Torvalds&#39; principles:</p><ol><li><strong>Good Taste</strong>: Eliminate special cases through proper data structure design</li><li><strong>Simplicity</strong>: Direct C API mapping with zero intermediate layers</li><li><strong>Practical</strong>: Solve real problems, not theoretical ones</li><li><strong>Memory Safety</strong>: Julia&#39;s GC + finalizers handle cleanup automatically</li></ol><h2 id="License"><a class="docs-heading-anchor" href="#License">License</a><a id="License-1"></a><a class="docs-heading-anchor-permalink" href="#License" title="Permalink"></a></h2><p>Licensed under the Apache License 2.0. See the source file headers for details.</p></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="api/">API Reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 28 November 2025 12:59">Friday 28 November 2025</span>. Using Julia version 1.12.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
