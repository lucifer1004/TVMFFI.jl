var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Module-and-Function-Management","page":"API Reference","title":"Module & Function Management","text":"Functions for loading modules and managing functions.","category":"section"},{"location":"api/#Tensors-and-Data","page":"API Reference","title":"Tensors & Data","text":"Core data structures for exchanging data with TVM.","category":"section"},{"location":"api/#Devices","page":"API Reference","title":"Devices","text":"Device management and creation.","category":"section"},{"location":"api/#GPU-Utilities","page":"API Reference","title":"GPU Utilities","text":"Helper functions for GPU support.","category":"section"},{"location":"api/#Types","page":"API Reference","title":"Types","text":"Core types used in the FFI.","category":"section"},{"location":"api/#Error-Handling","page":"API Reference","title":"Error Handling","text":"","category":"section"},{"location":"api/#Low-Level-API","page":"API Reference","title":"Low Level API","text":"Direct mappings to C API structures. Use TVMTensor and TVMFunction for high-level access.","category":"section"},{"location":"api/#TVMFFI","page":"API Reference","title":"TVMFFI","text":"TVMFFI\n\nJulia bindings for TVM FFI (Foreign Function Interface).\n\nThis package provides a Julia interface to TVM's C API, enabling machine learning model compilation and execution from Julia.\n\nCore Types\n\nDLDevice: Device abstraction (CPU, CUDA, etc.)\nDLDataType: Data type descriptor\nTVMError: Error handling\nTVMString: ABI-stable string type\nTVMFunction: Function objects\nTVMTensor: N-dimensional array type\n\nDesign Philosophy\n\nDirect C API calls via ccall (no intermediate layer)\nJulia's GC manages object lifetimes via finalizers\nSimple, clear abstractions without over-engineering\n\n\n\n\n\n","category":"module"},{"location":"api/#TVMFFI.load_module","page":"API Reference","title":"TVMFFI.load_module","text":"load_module(path::AbstractString) -> TVMModule\n\nLoad a compiled TVM module from file.\n\nExample\n\nmod = load_module(\"build/add_one_cpu.so\")\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.get_function","page":"API Reference","title":"TVMFFI.get_function","text":"get_function(mod::TVMModule, name, query_imports=true) -> Union{TVMFunction, Nothing}\n\nGet a function from the module by name.\n\nReturns nothing if function not found.\n\nExample\n\nmod = load_module(\"build/add_one_cpu.so\")\nadd_one = get_function(mod, \"add_one_cpu\")\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.system_lib","page":"API Reference","title":"TVMFFI.system_lib","text":"system_lib(symbol_prefix::AbstractString = \"\") -> TVMModule\n\nGet the system library module containing statically linked symbols.\n\nThe system library contains symbols that are registered via the C API during static initialization. This is useful for:\n\nAccessing statically compiled TVM modules\nTesting with symbols registered at compile time\nEmbedded systems without dynamic loading\n\nArguments\n\nsymbol_prefix: Optional prefix for symbol filtering (default: \"\")\n\nExamples\n\n# Get the system library\nmod = system_lib()\n\n# With prefix filtering\ntest_mod = system_lib(\"testing.\")  # Only symbols prefixed with \"__tvm_ffi_testing.\"\n\nSee Also\n\nPython equivalent: tvm_ffi.system_lib() Rust equivalent: SystemLib::new()\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.write_to_file","page":"API Reference","title":"TVMFFI.write_to_file","text":"write_to_file(mod::TVMModule, file_name::AbstractString, format::AbstractString = \"\")\n\nSave a TVM module to a file.\n\nArguments\n\nmod: The TVM module to save\nfile_name: Output file path\nformat: Optional format specifier (e.g., \"so\", \"dll\", \"dylib\")\n\nExamples\n\nmod = load_module(\"my_module.so\")\nwrite_to_file(mod, \"output.so\", \"so\")\n\nSee Also\n\nPython equivalent: mod.save(file_name, fmt)\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.inspect_source","page":"API Reference","title":"TVMFFI.inspect_source","text":"inspect_source(mod::TVMModule, format::AbstractString = \"\") -> String\n\nInspect the source code of a module for debugging.\n\nArguments\n\nmod: The TVM module to inspect\nformat: Optional format specifier (e.g., \"ll\" for LLVM IR, \"asm\" for assembly)\n\nExamples\n\nmod = load_module(\"my_module.so\")\nsource = inspect_source(mod, \"ll\")\nprintln(source)\n\nSee Also\n\nPython equivalent: mod.get_source(fmt)\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.get_module_kind","page":"API Reference","title":"TVMFFI.get_module_kind","text":"get_module_kind(mod::TVMModule) -> String\n\nGet the kind/type of a module (e.g., \"llvm\", \"cuda\", \"c\").\n\nExamples\n\nmod = load_module(\"my_module.so\")\nkind = get_module_kind(mod)\nprintln(\"Module kind: \", kind)\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.implements_function","page":"API Reference","title":"TVMFFI.implements_function","text":"implements_function(mod::TVMModule, name::AbstractString, query_imports::Bool = true) -> Bool\n\nCheck if a module implements a specific function.\n\nExamples\n\nmod = load_module(\"my_module.so\")\nif implements_function(mod, \"my_func\")\n    println(\"Function exists!\")\nend\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.get_global_func","page":"API Reference","title":"TVMFFI.get_global_func","text":"get_global_func(name::AbstractString) -> Union{TVMFunction, Nothing}\n\nGet a global function by name.\n\nArguments\n\nname::AbstractString: The name of the global function\n\nReturns\n\nTVMFunction if the function exists\nnothing if the function does not exist\n\nExamples\n\nfunc = get_global_func(\"my_custom_function\")\nif func !== nothing\n    result = func(arg1, arg2)\nend\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.register_global_func","page":"API Reference","title":"TVMFFI.register_global_func","text":"register_global_func(name::AbstractString, func::Function; override::Bool=false)\n\nRegister a Julia function as a global TVM function.\n\nArguments\n\nname: Global function name (e.g., \"mypkg.myfunc\")\nfunc: Julia function to register\noverride: Whether to override existing function\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.getindex","page":"API Reference","title":"Base.getindex","text":"Base.getindex(mod::TVMModule, name::AbstractString) -> TVMFunction\n\nGet a function from module using bracket notation (Python-style).\n\nExamples\n\nmod = load_module(\"build/add_one_cpu.so\")\nadd_one = mod[\"add_one_cpu\"]  # Cleaner!\nadd_one(x, y)\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.TVMTensor","page":"API Reference","title":"TVMFFI.TVMTensor","text":"TVMTensor\n\nTVM tensor wrapper with automatic memory management.\n\nProvides accessors for shape, dtype, and device information.\n\n\n\n\n\n","category":"type"},{"location":"api/#TVMFFI.shape","page":"API Reference","title":"TVMFFI.shape","text":"shape(tensor::TVMTensor) -> Vector{Int64}\n\nGet the shape of the tensor as a vector.\n\nNote\n\nJulia arrays typically use size() which returns a tuple. This function returns a vector for compatibility with some use cases.\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.dtype","page":"API Reference","title":"TVMFFI.dtype","text":"dtype(tensor::TVMTensor) -> DLDataType\n\nGet the data type of the tensor.\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.device","page":"API Reference","title":"TVMFFI.device","text":"device(tensor::TVMTensor) -> DLDevice\n\nGet the device of the tensor.\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.to_julia_array","page":"API Reference","title":"TVMFFI.to_julia_array","text":"to_julia_array(tensor::TVMTensor, ::Type{T}) -> Array{T}\n\nConvert TVM tensor to Julia array (zero-copy for CPU contiguous tensors).\n\nReturns an array that shares memory with the tensor.\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.from_julia_array","page":"API Reference","title":"TVMFFI.from_julia_array","text":"from_julia_array(arr, device=cpu()) -> DLTensorHolder\n\nCreate DLTensor holder from Julia array or slice.\n\nSupports CPU and GPU arrays, including zero-copy slices.\n\nExamples\n\n# Array\nholder = from_julia_array(x)\n\n# Slice (zero-copy)\ncol = @view matrix[:, 2]\nholder = from_julia_array(col)\n\n# GPU array (auto-detects device)\ngpu_holder = from_julia_array(cuda_array)\n\n\n\n\n\nfrom_julia_array(arr::AbstractArray)\n\nExtended method for GPU arrays - uses same DLTensorHolder as CPU!\n\nDesign Philosophy (Linus-style)\n\nONE function, ONE type, ALL devices.\n\nAuto-detect GPU backend from array type\nCreate appropriate device automatically\nReturn unified DLTensorHolder\n\nNo fromgpuarray needed - it was a redundant special case!\n\nExamples\n\nusing CUDA\n\n# CPU and GPU - same API!\ncpu_holder = from_julia_array(cpu_arr)     # Auto: CPU device\ngpu_holder = from_julia_array(gpu_arr)     # Auto: CUDA device\n\n# Both return DLTensorHolder{T, S}\n# Device info is in holder.tensor.device\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.dtype_to_julia_type","page":"API Reference","title":"TVMFFI.dtype_to_julia_type","text":"dtype_to_julia_type(dtype::DLDataType) -> Type\n\nConvert DLDataType to corresponding Julia type.\n\nExamples\n\ndt = DLDataType(Float32)\nT = dtype_to_julia_type(dt)  # Returns Float32\n\ndt2 = DLDataType(\"int64\")\nT2 = dtype_to_julia_type(dt2)  # Returns Int64\n\nSupported Types\n\nFloat: Float16, Float32, Float64\nInt: Int8, Int16, Int32, Int64\nUInt: UInt8, UInt16, UInt32, UInt64\nBool\n\nThrows\n\nThrows an error for unsupported dtype codes or bit widths.\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.DLDataType","page":"API Reference","title":"TVMFFI.DLDataType","text":"DLDataType\n\nRe-export from LibTVMFFI for convenience. Represents a data type in DLPack format.\n\n\n\n\n\n","category":"type"},{"location":"api/#TVMFFI.DLDataTypeCode","page":"API Reference","title":"TVMFFI.DLDataTypeCode","text":"DLDataTypeCode\n\nData type code enum (Int, UInt, Float, etc.) matching DLPack specification.\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.size","page":"API Reference","title":"Base.size","text":"Base.size(tensor::TVMTensor) -> Tuple\n\nGet the size of the tensor as a tuple (Julia standard).\n\n\n\n\n\nBase.size(tensor::TVMTensor, dim::Int) -> Int\n\nGet the size of a specific dimension.\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.ndims","page":"API Reference","title":"Base.ndims","text":"Base.ndims(tensor::TVMTensor) -> Int\n\nGet the number of dimensions.\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.length","page":"API Reference","title":"Base.length","text":"Base.length(tensor::TVMTensor) -> Int\n\nGet the total number of elements.\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.DLDevice","page":"API Reference","title":"TVMFFI.DLDevice","text":"DLDevice\n\nRe-export from LibTVMFFI for convenience. Represents a device (CPU, GPU, etc.) and device ID.\n\n\n\n\n\n","category":"type"},{"location":"api/#TVMFFI.DLDeviceType","page":"API Reference","title":"TVMFFI.DLDeviceType","text":"DLDeviceType\n\nDevice type enum (CPU, CUDA, etc.) matching DLPack specification.\n\n\n\n\n\n","category":"type"},{"location":"api/#TVMFFI.cpu","page":"API Reference","title":"TVMFFI.cpu","text":"cpu(id::Integer=0) -> DLDevice\n\nCreate a CPU device context.\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.cuda","page":"API Reference","title":"TVMFFI.cuda","text":"cuda(id::Integer=0) -> DLDevice\n\nCreate a CUDA device context.\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.opencl","page":"API Reference","title":"TVMFFI.opencl","text":"opencl(id::Integer=0) -> DLDevice\n\nCreate an OpenCL device context.\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.vulkan","page":"API Reference","title":"TVMFFI.vulkan","text":"vulkan(id::Integer=0) -> DLDevice\n\nCreate a Vulkan device context.\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.metal","page":"API Reference","title":"TVMFFI.metal","text":"metal(id::Integer=0) -> DLDevice\n\nCreate a Metal device context.\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.rocm","page":"API Reference","title":"TVMFFI.rocm","text":"rocm(id::Integer=0) -> DLDevice\n\nCreate a ROCm device context.\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.supports_gpu_backend","page":"API Reference","title":"TVMFFI.supports_gpu_backend","text":"supports_gpu_backend(backend::Symbol) -> Bool\n\nCheck if a specific GPU backend is available.\n\nDesign Philosophy (Linus-style)\n\nOld code: Try to introspect Main module and call .functional() New code: Just check if the package exists in the current environment Simpler, no weird Main.CUDA hacks\n\nArguments\n\nbackend::Symbol: Backend to check (:CUDA, :ROCm, :Metal, :oneAPI)\n\nReturns\n\nBool: Whether the backend package is loaded\n\nExamples\n\nif supports_gpu_backend(:CUDA)\n    println(\"CUDA is available!\")\nend\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.list_available_gpu_backends","page":"API Reference","title":"TVMFFI.list_available_gpu_backends","text":"list_available_gpu_backends() -> Vector{Symbol}\n\nList all available and functional GPU backends.\n\nReturns\n\nVector{Symbol}: List of available backends\n\nExamples\n\nbackends = list_available_gpu_backends()\nprintln(\"Available GPU backends: \", backends)\n# Output: [:CUDA, :ROCm]  (depending on system)\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.print_gpu_info","page":"API Reference","title":"TVMFFI.print_gpu_info","text":"print_gpu_info()\n\nPrint information about available GPU backends and devices.\n\nExamples\n\nprint_gpu_info()\n# Output:\n# Available GPU Backends:\n#   ✓ CUDA (NVIDIA)\n#     • Device 0: NVIDIA GeForce RTX 3090\n#     • Device 1: NVIDIA GeForce RTX 3080\n#   ✓ ROCm (AMD)\n#     • Device 0: AMD Radeon RX 6900 XT\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.gpu_array_info","page":"API Reference","title":"TVMFFI.gpu_array_info","text":"gpu_array_info(arr)\n\nPrint diagnostic information about a GPU array.\n\nExample\n\nusing CUDA\nx = CUDA.CuArray(Float32[1, 2, 3])\ngpu_array_info(x)\n# Output:\n#   Backend: CUDA\n#   Device: 0\n#   Type: Float32\n#   Shape: (3,)\n#   Pointer: 0x...\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.TVMFunction","page":"API Reference","title":"TVMFFI.TVMFunction","text":"TVMFunction\n\nWrapper for TVM function objects with automatic argument conversion.\n\n\n\n\n\n","category":"type"},{"location":"api/#TVMFFI.TVMModule","page":"API Reference","title":"TVMFFI.TVMModule","text":"TVMModule\n\nWrapper for TVM module objects.\n\n\n\n\n\n","category":"type"},{"location":"api/#TVMFFI.TVMObject","page":"API Reference","title":"TVMFFI.TVMObject","text":"TVMObject\n\nBase wrapper for TVM FFI object handles with automatic memory management.\n\n\n\n\n\n","category":"type"},{"location":"api/#TVMFFI.TVMError","page":"API Reference","title":"TVMFFI.TVMError","text":"TVMError <: Exception\n\nTVM FFI error type.\n\nFields\n\nkind::String: Error kind (ValueError, TypeError, etc.)\nmessage::String: Error message\nbacktrace::String: Stack backtrace\n\n\n\n\n\n","category":"type"},{"location":"api/#TVMFFI.TVMString","page":"API Reference","title":"TVMFFI.TVMString","text":"TVMString\n\nTVM ABI-stable string type.\n\nWraps TVM FFI string representation with automatic memory management. Small strings (≤7 bytes) are stored inline, larger strings are heap-allocated.\n\n\n\n\n\n","category":"type"},{"location":"api/#TVMFFI.TVMBytes","page":"API Reference","title":"TVMFFI.TVMBytes","text":"TVMBytes\n\nTVM ABI-stable bytes type (for binary data).\n\nSimilar to TVMString but for arbitrary binary data.\n\n\n\n\n\n","category":"type"},{"location":"api/#TVMFFI.register_object","page":"API Reference","title":"TVMFFI.register_object","text":"register_object(type_key::String, T::Type; parent_type_index::Int32 = Int32(64))\n\nRegister a Julia type as a TVM object type.\n\nArguments\n\ntype_key: The type key defined in C++ (e.g., \"tvm.RelayExpr\")\nT: The Julia type to map to this type key\nparent_type_index: Parent type index (default: 64 = ffi.Object)\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.get_type_index","page":"API Reference","title":"TVMFFI.get_type_index","text":"get_type_index(type_key::String) -> Int32\n\nGet the type index for a given type key.\n\n\n\n\n\n","category":"function"},{"location":"api/#Core.String","page":"API Reference","title":"Core.String","text":"Base.String(s::TVMString) -> String\n\nConvert TVMString to Julia String.\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.Vector","page":"API Reference","title":"Base.Vector","text":"Vector{UInt8}(b::TVMBytes) -> Vector{UInt8}\n\nConvert TVMBytes to Julia byte vector.\n\n\n\n\n\n","category":"type"},{"location":"api/#TVMFFI.check_call","page":"API Reference","title":"TVMFFI.check_call","text":"check_call(ret::Integer)\n\nCheck C API return code and throw TVMError if non-zero.\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.TVMErrorKind","page":"API Reference","title":"TVMFFI.TVMErrorKind","text":"TVMErrorKind\n\nError kind wrapper. Simply wraps a string for clarity.\n\n\n\n\n\n","category":"type"},{"location":"api/#TVMFFI.DLTensor","page":"API Reference","title":"TVMFFI.DLTensor","text":"DLTensor\n\nDLPack tensor structure (from dlpack.h). This is a C-compatible struct representing a multi-dimensional array.\n\nNote on GPU Pointers\n\nFor GPU arrays (CuArray, ROCArray, etc.), the data field contains a GPU device pointer, not a CPU pointer. We use UInt64 to store the pointer value and reinterpret it as needed, since GPU pointers can't be directly converted to Ptr{Cvoid}.\n\n\n\n\n\n","category":"type"},{"location":"api/#TVMFFI.DLTensorHolder","page":"API Reference","title":"TVMFFI.DLTensorHolder","text":"DLTensorHolder{T, S}\n\nSelf-contained DLTensor holder with automatic lifetime management.\n\nWorks for both CPU and GPU arrays, including slices (SubArray).\n\nFields\n\ntensor::DLTensor: DLPack tensor structure\nshape::Vector{Int64}: Shape array\nstrides::Vector{Int64}: Strides array\nsource::S: Source array (Array, CuArray, SubArray, etc.)\n\n\n\n\n\n","category":"type"},{"location":"api/#TVMFFI.tvm_ffi_version","page":"API Reference","title":"TVMFFI.tvm_ffi_version","text":"tvm_ffi_version() -> VersionNumber\n\nGet the TVM FFI version as a Julia VersionNumber.\n\nThis function queries the C API for version information and converts it to Julia's standard version type for easy comparison and display.\n\nExamples\n\njulia> v = tvm_ffi_version()\nv\"0.1.2\"\n\njulia> v >= v\"0.1.0\"\ntrue\n\n\n\n\n\n","category":"function"},{"location":"api/#TVMFFI.LibTVMFFI","page":"API Reference","title":"TVMFFI.LibTVMFFI","text":"LibTVMFFI\n\nLow-level C API bindings for TVM FFI.\n\nThis module provides direct Julia bindings to the TVM FFI C API. All functions follow the naming convention from c_api.h.\n\nDesign Notes\n\nUses ccall for direct C function invocation\nMatches C struct layouts exactly with Julia struct definitions\nNo intermediate abstractions - keep it simple and direct\n\n\n\n\n\n","category":"module"},{"location":"#TVMFFI.jl","page":"Home","title":"TVMFFI.jl","text":"Julia bindings for the TVM Deep Learning Compiler FFI.","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"TVMFFI.jl provides a bridge between Julia and the TVM runtime. It allows you to:\n\nLoad compiled TVM modules (CPU, CUDA, Metal, etc.).\nManage TVM arrays (TVMTensor) with zero-copy where possible.\nCall TVM functions from Julia.\nRegister Julia functions to be called by TVM.","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(\"TVMFFI\")","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"#Loading-a-Module","page":"Home","title":"Loading a Module","text":"using TVMFFI\n\n# 1. Load a compiled library\n# mod = load_module(\"compiled_lib.so\")\n\n# 2. Get a function\n# func = mod[\"my_function\"]\n\n# 3. Create input tensor\ninput = TVMTensor(Float32[1, 2, 3])\noutput = TVMTensor(zeros(Float32, 3))\n\n# 4. Call the function\n# func(input, output)","category":"section"},{"location":"#Working-with-GPU","page":"Home","title":"Working with GPU","text":"If you have a CUDA-enabled TVM build:\n\nusing TVMFFI, CUDA\n\n# Create a TVMTensor from a CuArray (zero-copy)\ncu_arr = CuArray(Float32[1, 2, 3])\ntvm_gpu_arr = TVMTensor(cu_arr)\n\n# Use it in TVM functions\n# func(tvm_gpu_arr)","category":"section"}]
}
